{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1657808861803,
     "user": {
      "displayName": "長曽我部崇",
      "userId": "09520727656816344536"
     },
     "user_tz": -540
    },
    "id": "m6md4V9lc77A",
    "outputId": "c79da3c7-0f9a-4649-e8d6-7c63d82e7c7b"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/My Drive/ColabNotebooks/mnist\n",
    "# %ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1657808862102,
     "user": {
      "displayName": "長曽我部崇",
      "userId": "09520727656816344536"
     },
     "user_tz": -540
    },
    "id": "tZzoAenebOYI",
    "outputId": "1b291681-bd60-4b6b-82cc-44efcf2debff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.11.0\n",
      "Torchvision Version:  0.12.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makedir(dir):\n",
    "  if not os.path.exists(dir):\n",
    "    os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1657808862102,
     "user": {
      "displayName": "長曽我部崇",
      "userId": "09520727656816344536"
     },
     "user_tz": -540
    },
    "id": "jGnoVfyYbOYL"
   },
   "outputs": [],
   "source": [
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"D:\\HDD_Study\\HDD_Github\\Gesture_Game_lerning\"\n",
    "# data_dir = \"C:\\ex\\sen\\data\\max_square\\\\train_val\"\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"resnet\"\n",
    "#結果出力ディレクトリ\n",
    "output_name = \"model\"\n",
    "# Number of classes in the dataset\n",
    "num_classes = 3\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 128\n",
    "# Number of epochs to train for\n",
    "num_epochs = 10\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = False\n",
    "\n",
    "# last or best\n",
    "last = False\n",
    "\n",
    "output_base_dir = \"C:\\\\Users\\\\H3rb\\\\source\\\\GitHub\\\\HandGesture_Game\"\n",
    "out_dir = os.path.join(output_base_dir,output_name)\n",
    "# if os.path.exists(out_dir):\n",
    "#   print(\"出力ディレクトリ名が被っているよ\")\n",
    "#   sys.exit()\n",
    "\n",
    "makedir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1657808862494,
     "user": {
      "displayName": "長曽我部崇",
      "userId": "09520727656816344536"
     },
     "user_tz": -540
    },
    "id": "6X_zSZN6bOYM"
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, last, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    last_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    ###########\n",
    "                    # 追加\n",
    "                    ###########\n",
    "                    # print(\"preds\")\n",
    "                    # print(preds)\n",
    "                    # print(\"label\")\n",
    "                    # print(labels)\n",
    "\n",
    "\n",
    "                    ###########\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            #######\n",
    "            # 追加\n",
    "            #######\n",
    "            # if phase == 'val':\n",
    "            #     print('{} / {} = Acc: {:.4f}'.format(running_corrects.double(), len(dataloaders[phase].dataset), epoch_acc))\n",
    "\n",
    "            #     print(preds)\n",
    "\n",
    "            #     print(labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_acc_history.append(epoch_acc)\n",
    "            #######\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "            \n",
    "            last_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    last_model_wts = copy.deepcopy(model.state_dict())\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    if last:\n",
    "        model.load_state_dict(last_model_wts)\n",
    "    else:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, train_acc_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1657808862494,
     "user": {
      "displayName": "長曽我部崇",
      "userId": "09520727656816344536"
     },
     "user_tz": -540
    },
    "id": "lew0Q69TbOYO"
   },
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1657808862495,
     "user": {
      "displayName": "長曽我部崇",
      "userId": "09520727656816344536"
     },
     "user_tz": -540
    },
    "id": "qYJ_5jXobOYP",
    "outputId": "50be73cc-8d1f-4305-8bc7-7aa31f9d68a3"
   },
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18('ResNet18_Weights.IMAGENET1K_V1')\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"efficientnetb0\":\n",
    "        model_ft = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft._fc.in_features\n",
    "        model_ft._fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "    \n",
    "    elif model_name == \"efficientnetb6\":\n",
    "        model_ft = EfficientNet.from_pretrained('efficientnet-b6')\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft._fc.in_features\n",
    "        model_ft._fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 528\n",
    "    \n",
    "    elif model_name == \"efficientnetb7\":\n",
    "        model_ft = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft._fc.in_features\n",
    "        model_ft._fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 600\n",
    "    elif model_name == \"efficientnetv2s\":\n",
    "        model_ft = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft._fc.in_features\n",
    "        model_ft._fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 480\n",
    "    elif model_name == \"efficientnetv2m\":\n",
    "        model_ft = EfficientNet.from_pretrained('efficientnet-b7')\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft._fc.in_features\n",
    "        model_ft._fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 480\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG11_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3\n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "\n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "# print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1802,
     "status": "ok",
     "timestamp": 1657808864293,
     "user": {
      "displayName": "長曽我部崇",
      "userId": "09520727656816344536"
     },
     "user_tz": -540
    },
    "id": "zVGslsQEbOYQ",
    "outputId": "8f7460af-5ce6-492d-b4e6-032a58b9e13f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         transforms.RandomResizedCrop(input_size),\n",
    "#         # transforms.RandomHorizontalFlip(),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     'val': transforms.Compose([\n",
    "#         transforms.Resize(input_size),\n",
    "#         transforms.CenterCrop(input_size),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "# }\n",
    "#手袋\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "# data_transforms = {\n",
    "#     'train': transforms.Compose([\n",
    "#         transforms.Resize(input_size),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "\n",
    "#         #データ拡張実験\n",
    "#         transforms.RandomVerticalFlip(),\n",
    "#         transforms.RandomRotation(degrees=(0,360),expand=True),\n",
    "#         transforms.Resize(input_size),\n",
    "\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "#     'val': transforms.Compose([\n",
    "#         transforms.Resize(input_size),\n",
    "#         transforms.CenterCrop(input_size),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "#     ]),\n",
    "# }\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "# Create training and validation dataloaders\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1820,
     "status": "ok",
     "timestamp": 1657808866106,
     "user": {
      "displayName": "長曽我部崇",
      "userId": "09520727656816344536"
     },
     "user_tz": -540
    },
    "id": "2eD1-mVmGyaE",
    "outputId": "cdd08b6f-c3ca-4d4f-acf4-4e158edeaf72"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4246,
     "status": "ok",
     "timestamp": 1657808870347,
     "user": {
      "displayName": "長曽我部崇",
      "userId": "09520727656816344536"
     },
     "user_tz": -540
    },
    "id": "uGrtxyOmbOYQ",
    "outputId": "e1a96393-989e-43e2-c6f8-917596b868df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t layer1.0.conv1.weight\n",
      "\t layer1.0.bn1.weight\n",
      "\t layer1.0.bn1.bias\n",
      "\t layer1.0.conv2.weight\n",
      "\t layer1.0.bn2.weight\n",
      "\t layer1.0.bn2.bias\n",
      "\t layer1.1.conv1.weight\n",
      "\t layer1.1.bn1.weight\n",
      "\t layer1.1.bn1.bias\n",
      "\t layer1.1.conv2.weight\n",
      "\t layer1.1.bn2.weight\n",
      "\t layer1.1.bn2.bias\n",
      "\t layer2.0.conv1.weight\n",
      "\t layer2.0.bn1.weight\n",
      "\t layer2.0.bn1.bias\n",
      "\t layer2.0.conv2.weight\n",
      "\t layer2.0.bn2.weight\n",
      "\t layer2.0.bn2.bias\n",
      "\t layer2.0.downsample.0.weight\n",
      "\t layer2.0.downsample.1.weight\n",
      "\t layer2.0.downsample.1.bias\n",
      "\t layer2.1.conv1.weight\n",
      "\t layer2.1.bn1.weight\n",
      "\t layer2.1.bn1.bias\n",
      "\t layer2.1.conv2.weight\n",
      "\t layer2.1.bn2.weight\n",
      "\t layer2.1.bn2.bias\n",
      "\t layer3.0.conv1.weight\n",
      "\t layer3.0.bn1.weight\n",
      "\t layer3.0.bn1.bias\n",
      "\t layer3.0.conv2.weight\n",
      "\t layer3.0.bn2.weight\n",
      "\t layer3.0.bn2.bias\n",
      "\t layer3.0.downsample.0.weight\n",
      "\t layer3.0.downsample.1.weight\n",
      "\t layer3.0.downsample.1.bias\n",
      "\t layer3.1.conv1.weight\n",
      "\t layer3.1.bn1.weight\n",
      "\t layer3.1.bn1.bias\n",
      "\t layer3.1.conv2.weight\n",
      "\t layer3.1.bn2.weight\n",
      "\t layer3.1.bn2.bias\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t fc.weight\n",
      "\t fc.bias\n"
     ]
    }
   ],
   "source": [
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are\n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34398,
     "status": "ok",
     "timestamp": 1657808904739,
     "user": {
      "displayName": "長曽我部崇",
      "userId": "09520727656816344536"
     },
     "user_tz": -540
    },
    "id": "q6YXMm9-bOYR",
    "outputId": "5e724c41-f33c-455b-8baa-33c8a88a5786"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9\n",
      "----------\n",
      "train Loss: 0.2563 Acc: 0.9302\n",
      "val Loss: 0.0452 Acc: 0.9956\n",
      "\n",
      "Epoch 1/9\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "model_ft, train_acc_hist_tensor, val_acc_hist_tensor = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, last, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def tensor_to_np(inp):\n",
    "    \"imshow for Tesor\"\n",
    "    inp = inp.numpy().transpose((1,2,0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    return inp\n",
    "\n",
    "def false_img_save(pred, label, input, false_img_count):\n",
    "    pil_img = Image.fromarray(input)\n",
    "    makedir(out_dir + 'error/pred_' + str(class_names[pred.item()]) + '_label_' + str(class_names[label.item()]))\n",
    "    pil_img.save(out_dir + f'error/pred_{class_names[pred.item()]}_label_{class_names[label.item()]}/{false_img_count}.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "executionInfo": {
     "elapsed": 1846,
     "status": "ok",
     "timestamp": 1657809449522,
     "user": {
      "displayName": "長曽我部崇",
      "userId": "09520727656816344536"
     },
     "user_tz": -540
    },
    "id": "ghMEEQfBGtJ2",
    "outputId": "0e7dead1-9d02-44d9-d50f-5e42b8dcbfc5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "def val_model(model, dataloaders, optimizer):\n",
    "    false_img_count = 0\n",
    "    phase = 'val'\n",
    "    confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "    model.eval()\n",
    "    for inputs, labels in dataloaders[phase]:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.set_grad_enabled(phase == 'train'):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            #######################################################\n",
    "            for i in range(inputs.size()[0]):\n",
    "                if preds[i] != labels[i]:\n",
    "                    input = tensor_to_np(inputs.cpu().data[i])\n",
    "                    print(input)\n",
    "                    input *= 255\n",
    "                    input = input.astype(np.uint8)\n",
    "                    \n",
    "                    false_img_save(preds[i], labels[i], input, false_img_count)\n",
    "                    false_img_count += 1\n",
    "\n",
    "            #######################################################\n",
    "\n",
    "            for t_confusion_matrix, p_confusion_matrix in zip(labels.view(-1), preds.view(-1)):\n",
    "                confusion_matrix[t_confusion_matrix.long(), p_confusion_matrix.long()] += 1\n",
    "    \n",
    "    confusion_matrix_numpy = confusion_matrix.to('cpu').detach().numpy().copy()\n",
    "    df_cmx = pd.DataFrame(confusion_matrix_numpy, index=class_names, columns=class_names)\n",
    "    plt.figure(figsize = (12, 3))\n",
    "    sn.set(font_scale = 1)\n",
    "    sn.heatmap(df_cmx, annot=True, fmt='g', cmap='Blues')\n",
    "    plt.savefig(os.path.join(out_dir,\"confusion_matrix.png\"))\n",
    "    sn.set(font_scale = 1)\n",
    "\n",
    "val_model(model_ft, dataloaders_dict, optimizer_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_txtfile(data, outpath):\n",
    "  file = outpath\n",
    "  fileobj = open(file, \"w\", encoding = \"utf_8\")\n",
    "  for index , i in enumerate(data):\n",
    "    if index == len(data) - 1:\n",
    "      fileobj.write(f\"{i}\")\n",
    "    else:\n",
    "      fileobj.write(f\"{i},\")\n",
    "\n",
    "  fileobj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_hist = []\n",
    "val_acc_hist = []\n",
    "\n",
    "for i in train_acc_hist_tensor:\n",
    "  train_acc_hist.append(float(i.item()))\n",
    "\n",
    "for i in val_acc_hist_tensor:\n",
    "  val_acc_hist.append(float(i.item()))\n",
    "\n",
    "plt.plot(train_acc_hist)\n",
    "plt.title(\"train_accuracy- epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(out_dir,\"train_acc.png\"))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(val_acc_hist)\n",
    "plt.title(\"test_accuracy- epoch\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(out_dir,\"val_acc.png\"))\n",
    "plt.show()\n",
    "\n",
    "save_txtfile(train_acc_hist, os.path.join(out_dir,\"train_acc_hist.txt\"))\n",
    "save_txtfile(val_acc_hist, os.path.join(out_dir,\"val_acc_hist.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, dataloaders, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = fig.add_subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}  label: {}'\n",
    "                             .format(class_names[preds[j]], class_names[labels[j]]))\n",
    "                \n",
    "                print(inputs.cpu().data[j].size())\n",
    "                ax.imshow(tensor_to_np(inputs.cpu().data[j]))\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "visualize_model(model_ft, dataloaders_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ft.state_dict(), os.path.join(out_dir,'model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "finetuning.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d379beeeffdbdc665e060ccd8f0c64a12e889cd80cfca66d216f8adad98c9b40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
